{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Starter_code_Assignment_CNN_Skin_Cancer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDriIbfa5lwD"
      },
      "source": [
        "# Problem statement\n",
        "To build a CNN based model which can accurately detect melanoma. Melanoma is a type of cancer that can be deadly if not detected early. It accounts for 75% of skin cancer deaths. A solution which can evaluate images and alert the dermatologists about the presence of melanoma has the potential to reduce a lot of manual effort needed in diagnosis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RpUsRQwOOL72"
      },
      "source": [
        "This assignment uses a dataset of about 2357 images of skin cancer types. The dataset contains 9 sub-directories in each train and test subdirectories. The 9 sub-directories contains the images of 9 skin cancer types respectively."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTHkTvKsHZ8W"
      },
      "source": [
        "## Building a simple CNN model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvR7ppk77v31"
      },
      "source": [
        "#### **Task 1: Reading the data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JfcpIXQZN2Rh"
      },
      "source": [
        "Importing all the important libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WC8xCQuELWms"
      },
      "source": [
        "# Importing the required libraries\n",
        "import pathlib\n",
        "import os\n",
        "import PIL\n",
        "\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8BKCg1aEzqy"
      },
      "source": [
        "You are expected to load the data in the Colab environment by mounting the Google Drive. This will allow you to access the files from Google drive through Colab. \n",
        "\n",
        "Steps:\n",
        "1. Upload the dataset in your Google Drive in a separate folder. Avoid spaces in the name of the folder or the zip file.\n",
        "2. Mount the Google Drive using the code given below.\n",
        "3. Unzip the file to access images.\n",
        "4. Check the path for the datasets - train and test.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9qFntWNOKB1"
      },
      "source": [
        "Note: The code is commented for you to learn and then make edits to execute the command."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TYpVPmT5z7AP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1cb219e-e2f2-4199-9199-8368f25afd18"
      },
      "source": [
        "## If you are using the data by mounting the google drive, use the following:\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "## Ref:https://towardsdatascience.com/downloading-datasets-into-google-drive-via-google-colab-bcb1b30b0166"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KD4mPiCSOfxK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "159ff781-672c-4552-927c-10f83c264f04"
      },
      "source": [
        "# Your GDrive Directory\n",
        "!ls /content/gdrive/MyDrive"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 001.JPG\n",
            " 002.JPG\n",
            " 004.JPG\n",
            " 009.jpg\n",
            " 014.JPG\n",
            " 033.JPG\n",
            " 035.JPG\n",
            " 066.JPG\n",
            " 075.JPG\n",
            " 083.JPG\n",
            " 100.JPG\n",
            " 104.JPG\n",
            " 113.JPG\n",
            " 122.jpg\n",
            " 20210614104541.gdoc\n",
            " 20210614104541.pdf\n",
            " chicago-application-form.pdf\n",
            "'Colab Notebooks'\n",
            " image0001.tif\n",
            " IMG_1748.jpg\n",
            "'In Harihar Nager Goal.mp3'\n",
            "'Interview for Masters of Science in Machine Learning and Cloud with IITM and LJMU (eib-yxdd-sub - Aug 12, 2021).gjam'\n",
            "'Moose Lake Jamboree Game Schedule.gdoc'\n",
            "'Pradish Kuruvilla Mathew Stern Recom.gdoc'\n",
            "'Pradish Mathew.gslides'\n",
            " rev-rpt-01-11.gsheet\n",
            "'Saturn docs'\n",
            "\"saturnsys.com - Prethish Mathew's Dashboard Project Report.gsheet\"\n",
            " stationsrailway.gmap\n",
            "'Stock  Master.gsheet'\n",
            " Takeout\n",
            "'Who are the patters and Raters (1).docx.gdoc'\n",
            "'Who are the patters and Raters.docx.gdoc'\n",
            " wireframe.html\n",
            " www.prethish.com.gsite\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AUw-lQfRMPh4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0c64e32-632b-484c-c0c3-aff0e41e2617"
      },
      "source": [
        "# Unzipping the files\n",
        "# To do: Update the folder name and file name\n",
        "!unzip /content/gdrive/My\\ 'Drive/Colab Notebooks/CNN_assignment.zip'"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/gdrive/My Drive/Colab Notebooks/CNN_assignment.zip\n",
            "replace Skin cancer ISIC The International Skin Imaging Collaboration/Test/actinic keratosis/ISIC_0010512.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2mNDUchCGxCB"
      },
      "source": [
        "Check the path mentioned in the output: `Skin cancer ISIC The International Skin Imaging Collaboration`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iS0fZ7d8G1Tn"
      },
      "source": [
        "Provide the path for train and test images using pathlib library."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D57L-ovIKtI4"
      },
      "source": [
        "# Defining the path for train and test images\n",
        "## To do: Update the paths of the train and test dataset\n",
        "data_dir_train = pathlib.Path(\"Skin cancer ISIC The International Skin Imaging Collaboration/Train\")\n",
        "data_dir_test = pathlib.Path('Skin cancer ISIC The International Skin Imaging Collaboration/Test')"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGoopgogP-aD"
      },
      "source": [
        "Check whether the data has been extracted successfully."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DqksN1w5Fu-N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b32cf7f3-a1a0-488e-a5fc-5974962d555e"
      },
      "source": [
        "# Train and test images\n",
        "image_count_train = len(list(data_dir_train.glob('*/*.jpg')))\n",
        "print(\"Train images:\\t\", image_count_train)\n",
        "image_count_test = len(list(data_dir_test.glob('*/*.jpg')))\n",
        "print(\"Test images:\\t\", image_count_test)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train images:\t 2239\n",
            "Test images:\t 118\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8HkfW3jPJun"
      },
      "source": [
        "#### **Task 2: Loading the images using keras.preprocessing**\n",
        "\n",
        "The images are still not loaded in the Colab environment. We have extracted them from the zip file. Now, you are expected to load them in the Colab notebook using the `preprocessing` attribute.\n",
        "\n",
        "Let's load these images off disk using the helpful image_dataset_from_directory utility. You can refer to the following [link](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image_dataset_from_directory) to know more about the function. You can even refer to the additional notebooks on the page to check how the variables are defined.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDBKZG3jPcMc"
      },
      "source": [
        "Defining the parameters for the loader:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLfcXcZ9LjGv"
      },
      "source": [
        "batch_size = 32\n",
        "img_height = 180\n",
        "img_width = 180"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5f5y43GPog1"
      },
      "source": [
        "Divide the training set into 2 parts: \n",
        "*   80% for training \n",
        "*   20% for validation\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1BWmDzr7w-5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4afae27d-a388-4877-b4d5-2c1274d137e6"
      },
      "source": [
        "## Write your code for train dataset here.\n",
        "## Note: Use seed=123 while creating your dataset using tf.keras.preprocessing.image_dataset_from_directory\n",
        "## Note: Make sure you resize your images to the size: img_height*img_width, while specifying the variable\n",
        "\n",
        "img_height = 180\n",
        "img_width = 180\n",
        "\n",
        "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "  data_dir_train,\n",
        "  validation_split=0.2,\n",
        "  subset=\"training\",\n",
        "  seed=123,\n",
        "  image_size=(img_height, img_width),\n",
        "  batch_size=batch_size)\n",
        "  "
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2239 files belonging to 9 classes.\n",
            "Using 1792 files for training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LYch6-SR-i2g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19a2326c-2592-4919-8da5-97d8f05cad27"
      },
      "source": [
        "## Write your code for validation dataset here.\n",
        "## Note: Use seed=123 while creating your dataset using tf.keras.preprocessing.image_dataset_from_directory\n",
        "## Note: Make sure you resize your images to the size: img_height*img_width, while specifying the variable\n",
        "\n",
        "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "  data_dir_test,\n",
        "  seed=123,\n",
        "  image_size=(img_height, img_width),\n",
        "  batch_size=batch_size)"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 118 files belonging to 9 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bk0RV7G7-nad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc2bf8e0-f49a-4cb8-b22d-ee4115c3a909"
      },
      "source": [
        "# List out all the classes of skin cancer and store them in a list. \n",
        "# You can find the class names in the 'class_names' attribute associated with the training and validation datasets. \n",
        "# These correspond to the directory names in alphabetical order.\n",
        "\n",
        "class_names = train_ds.class_names\n",
        "print(class_names)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['actinic keratosis', 'basal cell carcinoma', 'dermatofibroma', 'melanoma', 'nevus', 'pigmented benign keratosis', 'seborrheic keratosis', 'squamous cell carcinoma', 'vascular lesion']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cAZPYaeQjQy"
      },
      "source": [
        "#### **Task 3: Visualize the data**\n",
        "\n",
        "The `image_batch` is a tensor of the shape `(32, 180, 180, 3)`. This is a batch of 32 images of shape `180x180x3` (the last dimension refers to color channels RGB). The `label_batch` is a tensor of the shape `(32,)`, these are corresponding labels to the 32 images.\n",
        "\n",
        "Write the code to visualize one instance of all the nine classes present in the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKILZ48I-q1k"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "### Your code goes here; you can use either training or validation data to visualize the images.\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize = (10,10))\n",
        "for i in range(len(class_names)):\n",
        "    filtered_ds = train_ds.filter(lambda x, l: tf.math.equal(l[0], i))  \n",
        "    for image, label in filtered_ds.take(1):\n",
        "        ax = plt.subplot(3, 3, i+1)\n",
        "        plt.imshow(image[0].numpy().astype('uint8'))\n",
        "        plt.title(class_names[label.numpy()[0]])\n",
        "        plt.axis('off')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SH05xqE7pPba"
      },
      "source": [
        "#### Configure the dataset for performance\n",
        "Let's make sure to use buffered prefetching so you can yield data from disk without having I/O become blocking. These are two important methods you should use when loading data.\n",
        "\n",
        "`Dataset.cache()` keeps the images in memory after they're loaded off disk during the first epoch. This will ensure the dataset does not become a bottleneck while training your model. If your dataset is too large to fit into memory, you can also use this method to create a performant on-disk cache.\n",
        "\n",
        "`Dataset.prefetch()` overlaps data preprocessing and model execution while training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7wZlKRBEGNtU"
      },
      "source": [
        "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "\n",
        "train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
        "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1JEAF6-sRyz8"
      },
      "source": [
        "#### **Task 4: Create the model**\n",
        "Create a CNN model, which can accurately detect 9 classes present in the dataset. \n",
        "\n",
        "\n",
        "*   Note: The RGB channel values are in the `[0, 255]` range. This is not ideal for a neural network. Here, it is good to standardize values to be in the `[0, 1]`. Use `layers.experimental.preprocessing.Rescaling` for this purpose."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ync9xoW7GZgn"
      },
      "source": [
        "### Your code goes here\n",
        "model = Sequential()\n",
        "\n",
        "# Rescaling Layer\n",
        "model.add(tf.keras.layers.experimental.preprocessing.Rescaling(1./255, input_shape=(180, 180, 3)))\n",
        "\n",
        "# Convolution and Pooling layers\n",
        "model.add(layers.Conv2D(32, (3, 3), strides=(3,3), padding='valid', activation='relu'))\n",
        "model.add(layers.Conv2D(64, (3, 3), strides=(3,3), padding='valid',activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "\n",
        "# Flattening and Dense layers\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(128, activation='relu'))\n",
        "\n",
        "# Adding the output layer\n",
        "model.add(layers.Dense(10, activation='softmax'))\n",
        "\n",
        "# Model summary\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5h9Jp9mqEO4I"
      },
      "source": [
        "#### **Question**: \n",
        "Explain the following elements associated with the problem:\n",
        "1.   Selection of stride value (Reason for using a high/low value)\n",
        "2.   Padding strategy used (Same/Valid)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sdx5gALTFBjo"
      },
      "source": [
        "**Strides Reasoning : ** Since neighboring pixels are strongly correlated (specially in lowest layers), it makes sense to reduce the size of the output by subsampling (pooling) the filter response. The further apart two pixels are from each other, the less correlated. Therefore, a big stride in the pooling layer leads to high information loss. \n",
        "</br> **Padding strategy used (Same/Valid) : **  We used a padding stategy of same as we dont want to loss any information at the edges, so an extra padding is added to the right and left column so there is no information loss at the edges.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SDKzJmHwSCtt"
      },
      "source": [
        "#### **Task 5: Compile the model**\n",
        "Choose an appropirate optimiser and loss function for model training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XB8wKtiPGe1j"
      },
      "source": [
        "### Choose an appropirate optimiser and loss function\n",
        "model.compile(optimizer = 'adam',\n",
        "              loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ZGWN4MZGhtJ"
      },
      "source": [
        "# View the summary of all layers\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ljD_83rwSl5O"
      },
      "source": [
        "#### Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kkfw2rJXGlYC"
      },
      "source": [
        "# The model needs to be trained for 20 epochs\n",
        "epochs = 20\n",
        "\n",
        "history = model.fit(\n",
        "  train_ds,\n",
        "  validation_data = val_ds,\n",
        "  epochs=epochs\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3679V8OShSE"
      },
      "source": [
        "#### Visualizing training results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1xkgk5nGubz"
      },
      "source": [
        "# Graphs\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs_range = range(epochs)\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
        "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, loss, label='Training Loss')\n",
        "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JvPphJYuSZhK"
      },
      "source": [
        "#### **Question**:\n",
        "Write your findings after the model fit, see if there is an evidence of model overfit or underfit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3vRTPbJEn-pX"
      },
      "source": [
        "**Overfitting Model** The model  is overfitting as the Training Accuracy has increased to 72.35% while validation accuracy is lingering around 30%.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_0FHbmBw1bD"
      },
      "source": [
        "## Data augmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OGo9EzEgxwO2"
      },
      "source": [
        "Overfitting generally occurs when there are a small number of training examples. [Data augmentation](https://www.tensorflow.org/tutorials/images/data_augmentation) takes the approach of generating additional training data from your existing examples by augmenting them using random transformations that yield believable-looking images. This helps expose the model to more aspects of the data and generalize better."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22hljAl6GykA"
      },
      "source": [
        "# After you have analysed the model fit history for presence of underfit or overfit, choose an appropriate data augumentation strategy. \n",
        "\n",
        "data_augmentation = keras.Sequential(\n",
        "  [\n",
        "    layers.experimental.preprocessing.RandomFlip(\"horizontal\", \n",
        "                                                 input_shape=(img_height, \n",
        "                                                              img_width,\n",
        "                                                              3)),\n",
        "    layers.experimental.preprocessing.RandomRotation(0.1),\n",
        "    layers.experimental.preprocessing.RandomZoom(0.1),\n",
        "  ]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jf_Jm-gLC5pW"
      },
      "source": [
        "Let's visualize what a few augmented examples look like by applying data augmentation to the same image several times"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XEjPWh8GG0C7"
      },
      "source": [
        "# Visualizing how the augmentation strategy works for one instance of training image.\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "for images, _ in train_ds.take(1):\n",
        "  for i in range(9):\n",
        "    augmented_images = data_augmentation(images)\n",
        "    ax = plt.subplot(3, 3, i + 1)\n",
        "    plt.imshow(augmented_images[0].numpy().astype(\"uint8\"))\n",
        "    plt.axis(\"off\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XhKDHlUdTuSX"
      },
      "source": [
        "#### **Task 6: Create, compile and train the model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQMlf4NSDa6_"
      },
      "source": [
        "Model Definition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W3V4l-O9G3dM"
      },
      "source": [
        "## Your code goes here\n",
        "## You should also include dropouts to tackle with overfitting. (compulsory)\n",
        "### Your code goes here\n",
        "model = Sequential([data_augmentation])\n",
        "\n",
        "# Rescaling Layer\n",
        "model.add(tf.keras.layers.experimental.preprocessing.Rescaling(1./255, input_shape=(180, 180, 3)))\n",
        "\n",
        "# Convolution and Pooling layers\n",
        "model.add(layers.Conv2D(32, (3, 3), strides=(3,3), padding='valid', activation='relu'))\n",
        "model.add(layers.Conv2D(64, (3, 3), strides=(3,3), padding='valid',activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Dropout(0.25))\n",
        "\n",
        "# Flattening and Dense layers\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(128, activation='relu'))\n",
        "model.add(layers.Dropout(0.5))\n",
        "# Adding the output layer\n",
        "model.add(layers.Dense(10, activation='softmax'))\n",
        "\n",
        "# Model summary\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FfUWFp96UIAN"
      },
      "source": [
        "Compiling the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-7yTm8IG8zR"
      },
      "source": [
        "## Your code goes here\n",
        "### Choose an appropirate optimiser and loss function\n",
        "model.compile(optimizer = 'adam',\n",
        "              loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kC-D_RWOURp6"
      },
      "source": [
        "Training the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UcPfkUASHBf9"
      },
      "source": [
        "# Note: Train your model for 20 epochs\n",
        "## Your code goes here\n",
        "\n",
        "epochs = 20\n",
        "\n",
        "history = model.fit(\n",
        "  train_ds,\n",
        "  validation_data = val_ds,\n",
        "  epochs=epochs\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IhNOKtSyUYzC"
      },
      "source": [
        "#### Visualizing the results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vjN_F4QxHIsh"
      },
      "source": [
        "# Graphs\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs_range = range(epochs)\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
        "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, loss, label='Training Loss')\n",
        "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-AUR_b7UcaK"
      },
      "source": [
        "#### **Question**:\n",
        "Write your findings after the model fit, see if there is an evidence of model overfit or underfit. Do you think there is some improvement now as compared to the previous model run?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TMlN9QD8Fndg"
      },
      "source": [
        "The overfitting has considerable gone down when we added preprocessing data augmentation layer and multiple  dropout layer for the changes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7Z8T-bfHMhj"
      },
      "source": [
        "## Distribution in the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7TdDi4u-VTkW"
      },
      "source": [
        "**Context:** Many times real life datasets can have class imbalance, one class can have proportionately higher number of samples compared to the others. Class imbalance can have a detrimental effect on the final model quality. Hence as a sanity check it becomes important to check what is the distribution of classes in the data.\n",
        "\n",
        "<br>\n",
        "\n",
        "#### **Task 7: Find the distribution of classes in the training dataset.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HAhwYgtTQRzq"
      },
      "source": [
        "## Your code goes here.\n",
        "\n",
        "\n",
        "imbalanceDict = dict()\n",
        "\n",
        "for i in class_names:\n",
        "  imbalanceDict[i] = []\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "\n",
        "for imgs, lbls in train_ds:\n",
        "  for i in range(32):\n",
        "    imbalanceDict[class_names[lbls[i]]].append(imgs[i].numpy().astype(\"uint8\"))\n",
        "\n",
        "for i in imbalanceDict:\n",
        "  imbalanceDict[i] = len(imbalanceDict[i]) \n",
        "\n",
        "fig = plt.figure()\n",
        "fig.set_figwidth(20)\n",
        "fig.set_figheight(10)\n",
        "\n",
        "plt.bar(range(len(imbalanceDict)), list(imbalanceDict.values()), align='center')\n",
        "plt.xticks(range(len(imbalanceDict)), list(imbalanceDict.keys()))\n",
        "plt.show()\n",
        "\n",
        "for i in class_names:\n",
        "  print(i)\n",
        "  print(imbalanceDict[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4csQL1dvO0b2"
      },
      "source": [
        "#### **Questions:**  \n",
        " - Which class has the least number of samples?\n",
        " - Which classes dominate the data in terms proportionate number of samples?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YjntGiR2JcSw"
      },
      "source": [
        "*Which class has the least number of samples?* **seborrheic keratosis**\n",
        "<br/>*Which classes dominate the data in terms proportionate number of samples?* **pigmented benign keratosis**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hb-stKyHPf8v"
      },
      "source": [
        "#### **Task 8: Rectifying the class imbalance**\n",
        "You can use a python package known as `Augmentor` (https://augmentor.readthedocs.io/en/master/) to add more samples across all classes so that none of the classes have very few samples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ItAg4rU-SzJh"
      },
      "source": [
        "!pip install Augmentor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZKzTe3zWL4O"
      },
      "source": [
        "To use `Augmentor`, the following general procedure is followed:\n",
        "\n",
        "1. Instantiate a `Pipeline` object pointing to a directory containing your initial image data set.<br>\n",
        "2. Define a number of operations to perform on this data set using your `Pipeline` object.<br>\n",
        "3. Execute these operations by calling the `Pipeline’s` `sample()` method.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Egt9EHjR-Dd"
      },
      "source": [
        "# Provide the path for the training dataset\n",
        "print(type(data_dir_train))\n",
        "path_to_training_dataset =  str(data_dir_train)\n",
        "import Augmentor\n",
        "for i in class_names:\n",
        "    p = Augmentor.Pipeline(path_to_training_dataset + i)\n",
        "    p.rotate(probability=0.7, max_left_rotation=10, max_right_rotation=10)\n",
        "    p.sample(500)\n",
        "    ## We are adding 500 samples per class to make sure that none of the classes are sparse."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CcBIFZGbWuFa"
      },
      "source": [
        "Augmentor has stored the augmented images in the output sub-directory of each of the sub-directories of skin cancer types. Lets take a look at total count of augmented images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "jxWcMqZhdRWz"
      },
      "source": [
        "# Count of images under each class after addition\n",
        "image_count_train = len(list(data_dir_train.glob('*/output/*.jpg')))\n",
        "print(image_count_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJ5KarKq4kWJ"
      },
      "source": [
        "Lets see the distribution of augmented data after adding new images to the original training data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6tODrYIY2nxJ"
      },
      "source": [
        "path_list = [x for x in glob(os.path.join(data_dir_train, '*','output', '*.jpg'))]\n",
        "path_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZvVdF7g3E1z"
      },
      "source": [
        "lesion_list_new = [os.path.basename(os.path.dirname(os.path.dirname(y))) for y in glob(os.path.join(data_dir_train, '*','output', '*.jpg'))]\n",
        "lesion_list_new"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "okcqVFAA2nxK"
      },
      "source": [
        "dataframe_dict_new = dict(zip(path_list_new, lesion_list_new))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "njzBxTNT2nxK"
      },
      "source": [
        "df2 = pd.DataFrame(list(dataframe_dict_new.items()),columns = ['Path','Label'])\n",
        "new_df = original_df.append(df2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5j45rmxd2nxK"
      },
      "source": [
        "new_df['Label'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9NirFBvGPmgI"
      },
      "source": [
        "So, now we have added 500 images to all the classes to maintain some class balance. We can add more images as we want to improve training process."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xyVfE8zCMCip"
      },
      "source": [
        "### **Task 9: Repeating the steps for balanced augmented data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hFcj1XgndRWz"
      },
      "source": [
        "batch_size = 32\n",
        "img_height = 180\n",
        "img_width = 180"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0haOU11Ey8ey"
      },
      "source": [
        "**Create a training dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4ZY11judRWz"
      },
      "source": [
        "# Provide the path here\n",
        "data_dir_train=\"path to directory with training data + data created using augmentor\"\n",
        "\n",
        "# Provide the subset value here\n",
        "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "  data_dir_train,\n",
        "  seed=123,\n",
        "  validation_split = 0.2,\n",
        "  subset = ## Choose the correct parameter value, so that only training data is refered to,\n",
        "  image_size=(img_height, img_width),\n",
        "  batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwNJVDuBP5kf"
      },
      "source": [
        "**Create a validation dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TX191d_3dRW0"
      },
      "source": [
        "# Provide the subset value here\n",
        "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "  data_dir_train,\n",
        "  seed=123,\n",
        "  validation_split = 0.2,\n",
        "  subset = ## Choose the correct parameter value, so that only validation data is refered to,\n",
        "  image_size=(img_height, img_width),\n",
        "  batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JaoWeOEpVjqH"
      },
      "source": [
        "**Create your model (make sure to include normalization)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ch0MuKvFVr7O"
      },
      "source": [
        "## Your code goes here\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bu5N9LxkVx1B"
      },
      "source": [
        "**Compile your model (Choose optimizer and loss function appropriately)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H47GWmLbdRW1"
      },
      "source": [
        "## Your code goes here\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gS-Y1bJV7uy"
      },
      "source": [
        "**Train your model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fcV6OdI4dRW1"
      },
      "source": [
        "# Note: Train your model for 30 epochs\n",
        "## Your code goes here\n",
        "\n",
        "history = # Code"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iuvfCTsBWLMp"
      },
      "source": [
        "**Visualize the model results**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lCTXwfkTdRW1"
      },
      "source": [
        "# Graphs\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs_range = range(epochs)\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
        "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, loss, label='Training Loss')\n",
        "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Way4lakC4_p0"
      },
      "source": [
        "### **Task 10: Analyzing the results** \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hn7YlVt2NAQN"
      },
      "source": [
        "#### **Question:**\n",
        "- Did you get rid of underfitting/overfitting from the model? \n",
        "- Did class rebalance help in the process?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cLj3p1TNAWk"
      },
      "source": [
        "`< Double click on the cell to write your answer here.>`"
      ]
    }
  ]
}